---
title: "Analysis of the Online News Popularity Dataset"
author: "Suprotik Debnath and Michael Lightfoot"
header-includes:
- \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
- \usepackage[labelsep=space]{caption}
output: 
  pdf_document: default
  word_document: default
  html_document: default
fontsize: 12pt
mainfont: Times New Roman
---


## Introduction

We are working with the Online News Popularity Dataset from the UCI Machine Learning Repository. You can find more information about this dataset [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity).

The goal of this analysis is to produce models in order to predict the number of shares a website gathers. We will break up this analysis by `data_channel`, which has 6 categories: 

- Lifestyle
- Entertainment
- Business
- Social Media
- Tech
- World

We will automate the model selection and analysis report creation across all data channels.

Looking at the data, after breaking it up by data channel, we are left with 51 predictive attributes to help us predict the `shares` variable. These variables have broad groups:

- Variables referencing various counts of different types of content withing the article. 
- Variables referencing the keywords within the article.
- Variables referencing statistics of shares in Mashable.
- Variables referencing day of publishing. 
- Variables referencing closeness to LDA of the topics (Latent Dirichlet Allocation)
- Variables referencing content polarity. 

We will likely focus heavily on the variables regarding LDA, statistics of shares, and the counts of different types of content.

We will utilize various methods to model the response, including random forest, boosted tree, and linear regression models. 

## Data

First, we will read in the data. Then, we will subset the data based on the variable of interest. We are going to create a new categorical variable called `weekday` that merges all the `weekday_is` variables and labels articles by what day they were published. We will also remove the `url` and `timedelta` variables for the sake of our analysis as those are non-predictive.

```{r, eval = T, message = F}
library(readr)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(scales)
library(caret)

#Read in data 
data <- read_csv("OnlineNewsPopularity/OnlineNewsPopularity.csv")
data <- as_tibble(data)

#Subset data
subdata <- data %>%
  mutate(
    weekday = case_when(
      weekday_is_monday == "1" ~ "Monday",
      weekday_is_tuesday == "1" ~ "Tuesday",
      weekday_is_wednesday == "1" ~ "Wednesday",
      weekday_is_thursday == "1" ~ "Thursday",
      weekday_is_friday == "1" ~ "Friday",
      weekday_is_saturday == "1" ~ "Saturday",
      weekday_is_sunday == "1" ~ "Sunday"
    ),
    channel = case_when(
      data_channel_is_lifestyle == "1" ~ "Lifestyle",
      data_channel_is_entertainment == "1" ~ "Entertainment",
      data_channel_is_bus == "1" ~ "Business",
      data_channel_is_socmed == "1" ~ "Social Media",
      data_channel_is_tech == "1" ~ "Tech",
      data_channel_is_world == "1" ~ "World"
    )
  ) %>%
  select(
    -c(
      url,
      timedelta,
      weekday_is_monday,
      weekday_is_tuesday,
      weekday_is_wednesday,
      weekday_is_thursday,
      weekday_is_friday,
      weekday_is_saturday,
      weekday_is_sunday,
      is_weekend,
      data_channel_is_lifestyle,
      data_channel_is_entertainment,
      data_channel_is_bus,
      data_channel_is_socmed,
      data_channel_is_tech,
      data_channel_is_world
    )
  )

data_life <- subdata %>% filter(channel == "Lifestyle")
data_life
```



## Summarizations

### Summary Statistics

We will create some summary statistics and plots to help us understand our data.
```{r}
summary(data_life$shares)
```

Correlation matrix of shares to variables
```{r}
cor((data_life[, unlist(lapply(data_life, is.numeric))]), data_life$shares)   
```

Print out table of summary statistics by `weekday`
```{r}
data_life %>% group_by(weekday) %>% summarise(avg = round(mean(shares)), med = median(shares), var = round(var(shares)))
```

### Summary Plots

```{r, eval = T}
plot1 <- ggplot(data_life, aes(x = n_tokens_content, y = shares, color = global_sentiment_polarity)) + geom_point()
plot1
```

```{r}
plot2 <- ggplot(data_life, aes(x=weekday, y=shares, fill=weekday)) + geom_bar(stat='identity') +  scale_y_continuous(labels = scales::comma) + ggtitle("Number of Shares by Day") + xlab("Weekday") + ylab("Number of Shares")  + theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
plot2
```

```{r}
plot3 <- ggplot(data_life, aes(x=kw_avg_avg, y=shares, color=weekday)) + geom_point(stat='identity') + ggtitle("Avg Keyword by Shares") + xlab("Average Keyword") + ylab("Shares") + theme(plot.title = element_text(hjust = 0.5))
plot3
```

```{r}
plot4 <- ggplot(data_life, aes(x=shares)) + geom_histogram() + ggtitle("Total Shares Spread") + xlab("Shares") + ylab("Count") + theme(plot.title = element_text(hjust = 0.5))
plot4
```



## Modeling

First, let's create the testing and training data needed to generate ensemble models. We are going to be using a 70/30 split for the data.
```{r}
train <- sample(1:nrow(data_life), size=nrow(data_life)*0.7)
test <- dplyr::setdiff(1:nrow(data_life), train)
LifeTrain <- data_life[train, ]
LifeTest <- data_life[test, ]
```

After splitting the data, we will be creating four unique models: two with linear regression and two ensemble models. 

### Four Fitted Models

A Linear Regression model measures the relationship between a number of predictors `x` and a dependent variable `y`. In simple linear regression, only one predictor variable is used. However, in more complex modeling using multiple linear regression, we use either polynomial regression to fit higher order terms or focusing on main/interaction effects and max combinations.
```{r}
mlrfit1 <- train(shares ~ kw_avg_avg + num_videos + num_imgs + num_videos*num_imgs, data = LifeTrain, method = "lm", trControl = trainControl(method = "cv", number = 5))
mlrfit1
```

```{r}
set.seed(123)
trctrl <- trainControl(method = "repeatedcv", number=5, repeats=3)
life_rf <- expand.grid(mtry=c(1:15))
rforest <- train(shares ~ ., 
               method='rf', 
               trControl=trctrl, 
               data=LifeTrain, 
               preProcess=c("center", "scale"),
               tuneGrid=life_rf)
#predict the values for our response variable and compare it to our testing data. 
rforest_pred <- predict(rforest, newdata = select(LifeTest, -shares))
#a frequency of how many of each response there is. 
rfpred <- confusionMatrix(rforest_pred, LifeTest$shares)
rfpred
```

## Comparison